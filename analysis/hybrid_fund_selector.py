# analysis/hybrid_fund_selector.py
"""
Hibrit Fon SeÃ§im Sistemi - HÄ±zlÄ± + KapsamlÄ± + Performans Optimizasyonu
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
import logging
import random
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from database.connection import DatabaseManager
from config.config import Config

class HybridFundSelector:
    """Hibrit fon seÃ§im ve analiz sistemi"""
    
    def __init__(self, db_manager: DatabaseManager, config: Config):
        self.db = db_manager
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Cache iÃ§in
        self._fund_cache = {}
        self._last_cache_update = None
        
    def load_funds_hybrid(self, 
                         quick_sample=150,      # HÄ±zlÄ± Ã¶rnekleme
                         detailed_analysis=30,   # DetaylÄ± analiz
                         include_top=True,       # BÃ¼yÃ¼k fonlarÄ± dahil et
                         use_cache=True) -> Tuple[List[str], List[str]]:
        """
        Hibrit yaklaÅŸÄ±m: HÄ±zlÄ± Ã¶rnekleme + BÃ¼yÃ¼k fonlar + DetaylÄ± analiz
        
        Returns:
            Tuple[aktif_fonlar, analiz_fonlarÄ±]
        """
        
        print(f"ğŸ¯ Hibrit Fon SeÃ§im Sistemi BaÅŸlatÄ±lÄ±yor...")
        start_time = time.time()
        
        try:
            # 1. TÃœM FONLARI AL
            all_funds = self.db.get_all_fund_codes()
            print(f"ğŸ“Š Toplam {len(all_funds)} fon bulundu")
            
            # 2. BÃœYÃœK FONLARI BELÄ°RLE (Ã–ncelikli)
            top_funds = []
            if include_top:
                print("ğŸ’° En bÃ¼yÃ¼k fonlar belirleniyor...")
                top_funds = self._get_top_funds_by_size(50)
                print(f"   ğŸ“ˆ {len(top_funds)} bÃ¼yÃ¼k fon belirlendi")
            
            # 3. STRATÄ°FÄ°ED SAMPLÄ°NG (Temsili Ã¶rnekleme)
            print("ğŸ² Temsili Ã¶rnekleme yapÄ±lÄ±yor...")
            representative_sample = self._stratified_sampling(all_funds, quick_sample)
            print(f"   ğŸ“Š {len(representative_sample)} temsili fon seÃ§ildi")
            
            # 4. FONLARI BÄ°RLEÅTÄ°R
            combined_funds = list(set(top_funds + representative_sample))
            print(f"ğŸ”— Toplam {len(combined_funds)} fon birleÅŸtirildi")
            
            # 5. AKTÄ°FLÄ°K KONTROLÃœ (Paralel)
            print("âš¡ Paralel aktiflik kontrolÃ¼...")
            active_funds = self._check_funds_activity_parallel(combined_funds)
            print(f"âœ… {len(active_funds)} aktif fon bulundu")
            
            # 6. DETAYLI ANALÄ°Z Ä°Ã‡Ä°N SEÃ‡IM
            analysis_funds = self._select_analysis_funds(active_funds, detailed_analysis)
            print(f"ğŸ” {len(analysis_funds)} fon detaylÄ± analize seÃ§ildi")
            
            elapsed = time.time() - start_time
            print(f"â±ï¸ Hibrit seÃ§im tamamlandÄ±: {elapsed:.1f} saniye")
            
            return active_funds, analysis_funds
            
        except Exception as e:
            self.logger.error(f"Hibrit seÃ§im hatasÄ±: {e}")
            # Fallback: Ä°lk 50 fon
            fallback_funds = all_funds[:50] if 'all_funds' in locals() else []
            return fallback_funds, fallback_funds[:20]
    
    def _get_top_funds_by_size(self, top_n: int = 50) -> List[str]:
        """En bÃ¼yÃ¼k fonlarÄ± bul (fon bÃ¼yÃ¼klÃ¼ÄŸÃ¼ne gÃ¶re)"""
        try:
            query = """
            SELECT fcode, AVG(fcapacity) as avg_size, COUNT(*) as data_count
            FROM tefasfunds 
            WHERE pdate >= CURRENT_DATE - INTERVAL '30 days'
            AND fcapacity > 1000000  -- 1M TL Ã¼stÃ¼
            GROUP BY fcode
            HAVING COUNT(*) >= 10     -- En az 10 gÃ¼n veri
            ORDER BY avg_size DESC
            LIMIT %s
            """
            
            result = self.db.execute_query(query.replace('%s', str(top_n)))
            
            if not result.empty:
                return result['fcode'].tolist()
            else:
                return []
                
        except Exception as e:
            self.logger.warning(f"BÃ¼yÃ¼k fon sorgusu hatasÄ±: {e}")
            return []
    
    def _stratified_sampling(self, all_funds: List[str], sample_size: int) -> List[str]:
        """Stratified sampling - Her harf grubundan temsili seÃ§im"""
        try:
            # FonlarÄ± ilk harflerine gÃ¶re grupla
            grouped_funds = {}
            for fund in all_funds:
                first_letter = fund[0] if fund else 'Z'
                if first_letter not in grouped_funds:
                    grouped_funds[first_letter] = []
                grouped_funds[first_letter].append(fund)
            
            # Her gruptan proporsiyonel seÃ§im
            selected_funds = []
            random.seed(42)  # TutarlÄ± sonuÃ§lar iÃ§in
            
            total_groups = len(grouped_funds)
            base_sample_per_group = sample_size // total_groups
            
            for letter, funds in grouped_funds.items():
                # Her gruptan en az 1, en fazla grup fonlarÄ±nÄ±n %50'si
                group_sample_size = min(
                    len(funds), 
                    max(1, min(base_sample_per_group, len(funds) // 2))
                )
                
                # Rastgele seÃ§im
                group_sample = random.sample(funds, group_sample_size)
                selected_funds.extend(group_sample)
            
            return selected_funds[:sample_size]  # Ä°stenen boyutta kes
            
        except Exception as e:
            self.logger.warning(f"Stratified sampling hatasÄ±: {e}")
            # Fallback: Random sampling
            return random.sample(all_funds, min(sample_size, len(all_funds)))
    
    def _check_funds_activity_parallel(self, funds: List[str], max_workers: int = 8) -> List[str]:
        """Paralel aktiflik kontrolÃ¼ - Ã‡OK HIZLI"""
        active_funds = []
        
        def check_single_fund(fcode: str) -> Optional[str]:
            """Tek fon aktiflik kontrolÃ¼"""
            try:
                # HÄ±zlÄ± sorgu - sadece son 5 kayÄ±t
                data = self.db.get_fund_price_history(fcode, 5)
                
                if not data.empty:
                    last_date = pd.to_datetime(data['pdate'].max())
                    days_ago = (datetime.now() - last_date).days
                    
                    if days_ago < 60:  # Son 60 gÃ¼nde aktif
                        return fcode
                
                return None
                
            except Exception:
                return None
        
        # Paralel iÅŸleme
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # TÃ¼m iÅŸleri baÅŸlat
            future_to_fund = {
                executor.submit(check_single_fund, fcode): fcode 
                for fcode in funds
            }
            
            # SonuÃ§larÄ± topla
            for future in as_completed(future_to_fund):
                result = future.result()
                if result:
                    active_funds.append(result)
        
        return active_funds
    
    def _select_analysis_funds(self, active_funds: List[str], count: int) -> List[str]:
        """DetaylÄ± analiz iÃ§in en iyi fonlarÄ± seÃ§"""
        if len(active_funds) <= count:
            return active_funds
        
        try:
            # HÄ±zlÄ± Ã¶n deÄŸerlendirme ile en promising fonlarÄ± seÃ§
            fund_scores = []
            
            for fcode in active_funds[:min(100, len(active_funds))]:  # Max 100 fon hÄ±zlÄ± test
                try:
                    # Ã‡ok hÄ±zlÄ± metrik - sadece son 10 gÃ¼n
                    data = self.db.get_fund_price_history(fcode, 10)
                    
                    if len(data) >= 5:
                        prices = data['price']
                        recent_return = (prices.iloc[-1] / prices.iloc[0] - 1) * 100
                        volatility = prices.std() / prices.mean() * 100
                        
                        # Basit skor: getiri/risk oranÄ±
                        score = recent_return / max(volatility, 1) if volatility > 0 else recent_return
                        
                        fund_scores.append({
                            'fcode': fcode,
                            'score': score,
                            'return': recent_return
                        })
                        
                except Exception:
                    continue
            
            if fund_scores:
                # En yÃ¼ksek skorlu fonlarÄ± seÃ§
                df = pd.DataFrame(fund_scores)
                selected = df.nlargest(count, 'score')['fcode'].tolist()
                
                # Eksik varsa random ekle
                if len(selected) < count:
                    remaining = [f for f in active_funds if f not in selected]
                    additional = random.sample(remaining, min(count - len(selected), len(remaining)))
                    selected.extend(additional)
                
                return selected
            else:
                # Fallback: Ä°lk N fonu
                return active_funds[:count]
                
        except Exception as e:
            self.logger.warning(f"Analiz fonu seÃ§imi hatasÄ±: {e}")
            return active_funds[:count]

# PERFORMANS OPTÄ°MÄ°ZASYONU Ä°Ã‡Ä°N GELÄ°ÅMÄ°Å SÄ°STEM

class HighPerformanceFundAnalyzer:
    """YÃ¼ksek performanslÄ± fon analiz sistemi - TÃœM FONLAR iÃ§in"""
    
    def __init__(self, db_manager: DatabaseManager, config: Config):
        self.db = db_manager
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    def analyze_all_funds_optimized(self, 
                                   batch_size: int = 100,
                                   max_workers: int = 16,
                                   use_bulk_queries: bool = True,
                                   cache_results: bool = True) -> Dict:
        """
        TÃœM FONLAR iÃ§in optimize edilmiÅŸ analiz
        
        HÄ±zlandÄ±rma Teknikleri:
        1. Bulk SQL sorgularÄ±
        2. Paralel iÅŸleme
        3. Vectorized hesaplamalar
        4. Intelligent caching
        5. Memory-efficient processing
        """
        
        print("ğŸš€ HIGH-PERFORMANCE ANALYSIS BAÅLATIYOR...")
        print("="*50)
        
        start_time = time.time()
        
        # 1. BULK VERÄ° Ã‡EKÄ°MÄ°
        print("ğŸ“Š 1. Bulk veri Ã§ekimi...")
        all_fund_data = self._bulk_fetch_fund_data()
        print(f"   âœ… {len(all_fund_data)} fon verisi yÃ¼klendi")
        
        # 2. VECTORÄ°ZED HESAPLAMALAR
        print("âš¡ 2. Vectorized hesaplamalar...")
        performance_metrics = self._vectorized_calculations(all_fund_data)
        print(f"   âœ… {len(performance_metrics)} fon hesaplandÄ±")
        
        # 3. PARALEL ANALÄ°Z
        print("ğŸ”„ 3. Paralel detay analizi...")
        detailed_results = self._parallel_detailed_analysis(
            performance_metrics, max_workers=max_workers
        )
        print(f"   âœ… {len(detailed_results)} detaylÄ± analiz")
        
        # 4. SONUÃ‡LARI BÄ°RLEÅTÄ°R
        print("ğŸ”— 4. SonuÃ§larÄ± birleÅŸtir...")
        final_results = self._combine_results(performance_metrics, detailed_results)
        
        elapsed = time.time() - start_time
        print(f"â±ï¸ TOPLAM SÃœRE: {elapsed:.1f} saniye")
        print(f"ğŸ“Š SANIYE BAÅINA: {len(final_results)/elapsed:.1f} fon/saniye")
        
        return final_results
    
    def _bulk_fetch_fund_data(self) -> Dict:
        """Bulk SQL ile tÃ¼m fon verilerini Ã§ek"""
        try:
            # TEK SORGU ile son 60 gÃ¼nÃ¼n tÃ¼m verisi
            query = """
            WITH recent_data AS (
                SELECT fcode, pdate, price,
                       ROW_NUMBER() OVER (PARTITION BY fcode ORDER BY pdate DESC) as rn
                FROM tefasfunds 
                WHERE pdate >= CURRENT_DATE - INTERVAL '90 days'
                AND price > 0
            )
            SELECT fcode, pdate, price
            FROM recent_data 
            WHERE rn <= 60  -- Son 60 kayÄ±t
            ORDER BY fcode, pdate DESC
            """
            
            print("   ğŸ“¡ Bulk SQL sorgusu Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
            all_data = self.db.execute_query(query)
            
            # Fon bazÄ±nda grupla
            fund_data = {}
            for fcode, group in all_data.groupby('fcode'):
                if len(group) >= 10:  # En az 10 gÃ¼n veri
                    fund_data[fcode] = group.sort_values('pdate')
            
            return fund_data
            
        except Exception as e:
            self.logger.error(f"Bulk fetch hatasÄ±: {e}")
            return {}
    
    def _vectorized_calculations(self, fund_data: Dict) -> pd.DataFrame:
        """Vectorized NumPy hesaplamalarÄ± - Ã‡OK HIZLI"""
        
        metrics_list = []
        
        for fcode, data in fund_data.items():
            try:
                prices = data['price'].values
                
                if len(prices) < 5:
                    continue
                
                # Vectorized hesaplamalar
                returns = np.diff(prices) / prices[:-1]
                
                # Temel metrikler
                total_return = (prices[-1] / prices[0] - 1) * 100
                annual_return = total_return * (252 / len(prices))
                volatility = np.std(returns) * np.sqrt(252) * 100
                sharpe = (annual_return - 15) / volatility if volatility > 0 else 0
                win_rate = np.sum(returns > 0) / len(returns) * 100
                
                # Max drawdown (vectorized)
                cumulative = np.cumprod(1 + returns)
                running_max = np.maximum.accumulate(cumulative)
                drawdowns = (cumulative - running_max) / running_max
                max_drawdown = np.min(drawdowns) * 100
                
                metrics_list.append({
                    'fcode': fcode,
                    'current_price': prices[-1],
                    'total_return': total_return,
                    'annual_return': annual_return,
                    'volatility': volatility,
                    'sharpe_ratio': sharpe,
                    'win_rate': win_rate,
                    'max_drawdown': max_drawdown,
                    'data_points': len(prices)
                })
                
            except Exception:
                continue
        
        return pd.DataFrame(metrics_list)
    
    def _parallel_detailed_analysis(self, metrics_df: pd.DataFrame, max_workers: int = 16) -> Dict:
        """Paralel detaylÄ± analiz"""
        
        def analyze_single_fund(row):
            """Tek fon iÃ§in detaylÄ± analiz"""
            try:
                fcode = row['fcode']
                
                # 2025 skoru hesapla
                score = self._calculate_2025_score_optimized(
                    row['annual_return'], 
                    row['volatility'], 
                    row['sharpe_ratio'], 
                    row['win_rate']
                )
                
                return {
                    'fcode': fcode,
                    'score_2025': score,
                    'category': self._get_score_category(score),
                    'recommendation': self._get_recommendation(score)
                }
                
            except Exception:
                return None
        
        detailed_results = {}
        
        # Paralel iÅŸleme
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(analyze_single_fund, row) 
                for _, row in metrics_df.iterrows()
            ]
            
            for future in as_completed(futures):
                result = future.result()
                if result:
                    detailed_results[result['fcode']] = result
        
        return detailed_results
    
    def _calculate_2025_score_optimized(self, annual_return, volatility, sharpe, win_rate):
        """Optimize edilmiÅŸ skor hesaplama"""
        # Vectorized hesaplama
        return_score = np.clip(annual_return / 30 * 30, 0, 30)
        sharpe_score = np.clip(sharpe * 10, 0, 25)
        risk_score = np.clip(20 - np.abs(volatility - 20) / 2, 0, 25)
        consistency_score = np.clip(win_rate / 5, 0, 20)
        
        return return_score + sharpe_score + risk_score + consistency_score
    
    def _get_score_category(self, score):
        """Skor kategorisi"""
        if score >= 80: return "MÃ¼kemmel"
        elif score >= 70: return "Ã‡ok Ä°yi"
        elif score >= 60: return "Ä°yi"
        elif score >= 50: return "Orta"
        else: return "ZayÄ±f"
    
    def _get_recommendation(self, score):
        """YatÄ±rÄ±m Ã¶nerisi"""
        if score >= 80: return "GÃ¼Ã§lÃ¼ AlÄ±m"
        elif score >= 70: return "AlÄ±m"
        elif score >= 50: return "Bekle"
        else: return "Sat"
    
    def _combine_results(self, metrics_df: pd.DataFrame, detailed_results: Dict) -> pd.DataFrame:
        """SonuÃ§larÄ± birleÅŸtir"""
        
        # Detailed results'Ä± DataFrame'e Ã§evir
        detailed_df = pd.DataFrame(list(detailed_results.values()))
        
        # BirleÅŸtir
        if not detailed_df.empty:
            final_df = metrics_df.merge(detailed_df, on='fcode', how='inner')
        else:
            final_df = metrics_df.copy()
            final_df['score_2025'] = 50  # Default score
        
        # Skorlara gÃ¶re sÄ±rala
        final_df = final_df.sort_values('score_2025', ascending=False)
        
        return final_df

# HIZLANDIRMA STRATEJÄ°LERÄ° REHBERÄ°

def print_optimization_guide():
    """KapsamlÄ± analiz iÃ§in hÄ±zlandÄ±rma stratejileri"""
    
    guide = """
ğŸš€ KAPSAMLI ANALÄ°Z HIZLANDIRMA STRATEJÄ°LERÄ°
==========================================

ğŸ“Š 1. VERÄ°TABANI OPTÄ°MÄ°ZASYONU:
   â€¢ PostgreSQL indeksleri:
     CREATE INDEX idx_tefasfunds_fcode_pdate ON tefasfunds(fcode, pdate);
     CREATE INDEX idx_tefasfunds_recent ON tefasfunds(pdate) WHERE pdate >= CURRENT_DATE - INTERVAL '90 days';
   
   â€¢ Connection pooling (pgbouncer)
   â€¢ Bulk queries (tek sorguda tÃ¼m veri)
   â€¢ Prepared statements

âš¡ 2. PARALEL Ä°ÅLEME:
   â€¢ ThreadPoolExecutor (I/O iÅŸlemleri iÃ§in)
   â€¢ ProcessPoolExecutor (CPU yoÄŸun hesaplamalar iÃ§in)
   â€¢ Chunk processing (100'lÃ¼k gruplar)
   â€¢ Asenkron database queries

ğŸ§® 3. VECTORÄ°ZED HESAPLAMALAR:
   â€¢ NumPy array operations
   â€¢ Pandas vectorized functions
   â€¢ Avoid Python loops
   â€¢ Broadcasting operations

ğŸ’¾ 4. MEMORY OPTÄ°MÄ°ZASYONU:
   â€¢ Chunked processing
   â€¢ Data type optimization (float32 vs float64)
   â€¢ Memory mapping for large datasets
   â€¢ Garbage collection tuning

ğŸ—„ï¸ 5. CACHING STRATEJÄ°LERÄ°:
   â€¢ Redis for intermediate results
   â€¢ In-memory caching for repeated calculations
   â€¢ Disk-based caching for large datasets
   â€¢ Invalidation strategies

âš™ï¸ 6. SÄ°STEM OPTÄ°MÄ°ZASYONU:
   â€¢ SSD storage for database
   â€¢ Sufficient RAM (16GB+ recommended)
   â€¢ Multi-core CPU utilization
   â€¢ Network optimization

ğŸ“ˆ PERFORMANS BEKLENTÄ°LERÄ°:
==================
Sistem KonfigÃ¼rasyonu â†’ SÃ¼re (1793 fon iÃ§in):

ğŸ’» Temel Sistem (4 core, 8GB RAM):
   â€¢ Seri iÅŸleme: ~15-20 dakika
   â€¢ Paralel (hibrit): ~3-5 dakika
   â€¢ Optimize edilmiÅŸ: ~1-2 dakika

ğŸ–¥ï¸ GÃ¼Ã§lÃ¼ Sistem (8 core, 16GB+ RAM):
   â€¢ Hibrit yaklaÅŸÄ±m: ~1-2 dakika
   â€¢ Optimize edilmiÅŸ: ~30-60 saniye
   â€¢ Ã‡ok optimize: ~15-30 saniye

ğŸ–¥ï¸ Server SÄ±nÄ±fÄ± (16+ core, 32GB+ RAM):
   â€¢ Optimize edilmiÅŸ: ~10-20 saniye
   â€¢ Maximum optimization: ~5-10 saniye

ğŸ’¡ UYGULAMA Ã–NERÄ°LERÄ°:
=====================
1. Hibrit yaklaÅŸÄ±mla baÅŸla (1-2 dakika)
2. KullanÄ±cÄ± isteÄŸinde tam analiz (5-10 dakika)
3. GÃ¼nlÃ¼k batch processing (gece 1-2 saat)
4. SonuÃ§larÄ± cache'le (tekrar hÄ±zlÄ±)

ğŸ¯ Ã–NCELÄ°K SIRASI:
================
1. Database indeksleri â† EN Ã–NEMLÄ°
2. Bulk SQL queries
3. Paralel processing
4. Vectorized calculations
5. Caching
6. Memory optimization
"""
    
    print(guide)

# Hibrit sistemi interactive_qa_dual_ai.py'e entegre etme
def integrate_hybrid_system():
    """Hibrit sistemi mevcut Q&A sistemine entegre et"""
    
    integration_code = '''
    # interactive_qa_dual_ai.py'de _load_active_funds metodunu deÄŸiÅŸtir:
    
    def _load_active_funds(self, max_funds=None, mode="comprehensive"):
        """
        GeliÅŸmiÅŸ fon yÃ¼kleme sistemi
        mode: "hybrid", "comprehensive", "fast"
        """
        
        if mode == "hybrid":
            # Hibrit yaklaÅŸÄ±m (1-2 dakika)
            selector = HybridFundSelector(self.coordinator.db, self.config)
            active_funds, analysis_funds = selector.load_funds_hybrid(
                quick_sample=150,
                detailed_analysis=30,
                include_top=True
            )
            return analysis_funds  # Analiz iÃ§in optimize edilmiÅŸ liste
            
        elif mode == "comprehensive":
            # KapsamlÄ± analiz (5-10 dakika)
            analyzer = HighPerformanceFundAnalyzer(self.coordinator.db, self.config)
            all_results = analyzer.analyze_all_funds_optimized()
            return all_results.head(50)['fcode'].tolist()  # En iyi 50
            
        else:  # fast
            # HÄ±zlÄ± mod (30 saniye)
            all_funds = self.coordinator.db.get_all_fund_codes()
            return all_funds[:50]  # Ä°lk 50
    '''
    
    print("ğŸ”§ Entegrasyon Kodu:")
    print(integration_code)

if __name__ == "__main__":
    print_optimization_guide()
    print("\n" + "="*50)
    integrate_hybrid_system()